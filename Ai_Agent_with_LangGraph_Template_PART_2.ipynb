{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cc0a767b7d6147ddb9fb409a6e0bb57f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_68d37a8881ef4c36ace4251ddc406c3a",
              "IPY_MODEL_d34687c42e9b43ffa08e6f8c19d371a6",
              "IPY_MODEL_f67d0ab43743476ea33cc243e272ed39"
            ],
            "layout": "IPY_MODEL_1c2a0a2f33b14aee8c725de10d44861f"
          }
        },
        "68d37a8881ef4c36ace4251ddc406c3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1063b361530142a3bbbe0b477c326a13",
            "placeholder": "​",
            "style": "IPY_MODEL_f1c4f24e443044d08ec2924d11e8929c",
            "value": "Loading weights: 100%"
          }
        },
        "d34687c42e9b43ffa08e6f8c19d371a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89f95ffaa3fa42fc93145fcd3cbd8fce",
            "max": 103,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4edf3e1a37d04747b924d2205cb80712",
            "value": 103
          }
        },
        "f67d0ab43743476ea33cc243e272ed39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cfcdce785e24d80b97c41e51dd0bbdb",
            "placeholder": "​",
            "style": "IPY_MODEL_c6128875b00c49acb581f9ce35065ecf",
            "value": " 103/103 [00:00&lt;00:00, 497.51it/s, Materializing param=pooler.dense.weight]"
          }
        },
        "1c2a0a2f33b14aee8c725de10d44861f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1063b361530142a3bbbe0b477c326a13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1c4f24e443044d08ec2924d11e8929c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89f95ffaa3fa42fc93145fcd3cbd8fce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4edf3e1a37d04747b924d2205cb80712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1cfcdce785e24d80b97c41e51dd0bbdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6128875b00c49acb581f9ce35065ecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment2_Part2**"
      ],
      "metadata": {
        "id": "ws4aUuFjEaWh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1z2FTgwEQFH",
        "outputId": "fd49aa67-5eb2-43a9-dd71-e5e2b53a49ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.8)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.10)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.2.13)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.3)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.6)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.12.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.46)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.4)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.13.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.7.3)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (26.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.14.0)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (5.0.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.10.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.24.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.24.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph) (1.12.2)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (0.24.0)\n",
            "Requirement already satisfied: click>=8.2.1 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (8.3.1)\n",
            "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (0.0.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install langgraph langchain langchain-community langchain-core faiss-cpu sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before writing any agent code, all the required libraries need to be installed. Part II builds on Part I by adding FAISS for vector search and sentence-transformers for generating embeddings, which are the two core components that make the RAG pipeline work."
      ],
      "metadata": {
        "id": "Knzi6WU5E-c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y zstd\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5vd51Q5FN2t",
        "outputId": "5bba3948-00fa-4686-da00-a50c72ba1715"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "zstd is already the newest version (1.4.8+dfsg-3build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading ollama-linux-amd64.tar.zst\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this is a fresh Colab session, Ollama needs to be installed again from scratch. The zstd package is installed first because the newer version of Ollama requires it for extraction before the main install script can run successfully."
      ],
      "metadata": {
        "id": "XA1enHvPFRSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess, time\n",
        "\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "time.sleep(5)\n",
        "result = subprocess.run([\"ollama\", \"pull\", \"mistral\"], capture_output=False)\n",
        "print(\"Done!\", result.returncode)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vxKPVr8Guqm",
        "outputId": "9d46079d-c507-4648-b0de-51d711a95bea"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once Ollama is installed, the server needs to be started as a background process before any model can be used. The mistral model is then pulled so it is ready for the agent to use throughout Part 2, same as in Part 1."
      ],
      "metadata": {
        "id": "V5bmrkAkHEzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-ollama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZRQR_8pH4O0",
        "outputId": "3b2c0b94-e9c5-42ee-b364-aa3d0810e078"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-ollama in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-ollama) (1.2.13)\n",
            "Requirement already satisfied: ollama<1.0.0,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from langchain-ollama) (0.6.1)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.7.3)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (26.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (9.1.4)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.14.0)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama<1.0.0,>=0.6.0->langchain-ollama) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama<1.0.0,>=0.6.0->langchain-ollama) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.32.5)\n",
            "Requirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.6.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Optional\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "llm = ChatOllama(model=\"mistral\")"
      ],
      "metadata": {
        "id": "czpbtgKsHENR"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All necessary libraries are imported including the new ones needed for Part II. HuggingFaceEmbeddings converts text chunks into vectors, FAISS stores and searches those vectors, and RecursiveCharacterTextSplitter breaks the document into manageable chunks before embedding."
      ],
      "metadata": {
        "id": "uOdklsVAHw8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"\"\"\n",
        "Artificial Intelligence is the simulation of human intelligence by machines.\n",
        "Machine Learning is a subset of AI that allows systems to learn from data.\n",
        "Deep Learning uses neural networks with many layers to learn complex patterns.\n",
        "Natural Language Processing enables computers to understand human language.\n",
        "LangGraph is a framework for building stateful multi-agent AI applications.\n",
        "FAISS is a library for efficient similarity search on dense vectors.\n",
        "RAG stands for Retrieval Augmented Generation which combines search with LLM generation.\n",
        "Vector embeddings represent text as numerical arrays that capture semantic meaning.\n",
        "A stateful agent maintains context and memory across multiple interaction steps.\n",
        "Conditional edges in LangGraph allow dynamic routing based on agent decisions.\n",
        "\"\"\"\n",
        "\n",
        "with open(\"document.txt\", \"w\") as f:\n",
        "    f.write(sample_text)\n",
        "\n",
        "print(\"Document created successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_t9ti-d_ISuV",
        "outputId": "820566b0-7c27-4a05-ac9f-285d765622c4"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document created successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A sample text document is created and saved locally to serve as the knowledge base for the RAG pipeline. This document covers key AI and LangGraph concepts that the agent will retrieve from when answering document related questions."
      ],
      "metadata": {
        "id": "0qYHQOvEIcYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading and chunk the document\n",
        "with open(\"document.txt\", \"r\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=200,\n",
        "    chunk_overlap=20\n",
        ")\n",
        "\n",
        "chunks = text_splitter.create_documents([raw_text])\n",
        "print(f\"Total chunks created: {len(chunks)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mst2wEYIdqP",
        "outputId": "de831a5a-93e0-4bc2-c1d4-5f3b47afe99f"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks created: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The document is loaded and split into smaller chunks using RecursiveCharacterTextSplitter. Breaking the text into chunks is necessary because embedding the entire document at once would lose granularity, making it harder to retrieve the most relevant section for a given question."
      ],
      "metadata": {
        "id": "vjrICSxAIlMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating embeddings and build FAISS vector store\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "print(\"Vector store created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194,
          "referenced_widgets": [
            "cc0a767b7d6147ddb9fb409a6e0bb57f",
            "68d37a8881ef4c36ace4251ddc406c3a",
            "d34687c42e9b43ffa08e6f8c19d371a6",
            "f67d0ab43743476ea33cc243e272ed39",
            "1c2a0a2f33b14aee8c725de10d44861f",
            "1063b361530142a3bbbe0b477c326a13",
            "f1c4f24e443044d08ec2924d11e8929c",
            "89f95ffaa3fa42fc93145fcd3cbd8fce",
            "4edf3e1a37d04747b924d2205cb80712",
            "1cfcdce785e24d80b97c41e51dd0bbdb",
            "c6128875b00c49acb581f9ce35065ecf"
          ]
        },
        "id": "r7G8JsEmIuyg",
        "outputId": "8ecc9d70-02e4-4e41-ca3e-20b05789c168"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc0a767b7d6147ddb9fb409a6e0bb57f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each text chunk is converted into a numerical vector using the sentence-transformers model and stored in a FAISS index. This vector store is what allows the agent to find the most semantically similar chunks to any given question during retrival."
      ],
      "metadata": {
        "id": "fwAUuNicIuf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 1: Define Updated Agent State\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    question: str\n",
        "    decision: Optional[str]\n",
        "    tool_output: Optional[str]\n",
        "    retrieved_docs: Optional[list]\n",
        "    final_answer: Optional[str]"
      ],
      "metadata": {
        "id": "jZIknXJXJEuk"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The AgentState is updated from Part I to include a new retrieved_docs field which stores the documents fetched from the FAISS vector store during retrieval. The tool_input field is removed since Part II handles expression extraction differently through the RAG pipeline routing."
      ],
      "metadata": {
        "id": "gbqTfbTxJPjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 2: Create Calculator Tool\n",
        "\n",
        "import re\n",
        "\n",
        "@tool\n",
        "def calculator(expression: str) -> str:\n",
        "    \"\"\"\n",
        "    Evaluate a basic mathematical expression.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return str(eval(expression))\n",
        "    except Exception:\n",
        "        return \"Error in calculation.\""
      ],
      "metadata": {
        "id": "XUbIcYY5JrOt"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The calculator tool from Part I is carried over here unchanged. It takes a mathematical expression as a string and evaluates it using Python's eval() function, returning the result or an error message if the expression is invalid."
      ],
      "metadata": {
        "id": "42smWlddJzZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 3: Updated Decision Node\n",
        "\n",
        "def decision_node(state: AgentState) -> AgentState:\n",
        "    question = state[\"question\"].lower()\n",
        "\n",
        "    # Rule-based math detection\n",
        "    if any(char.isdigit() for char in question) and any(op in question for op in [\"+\", \"-\", \"*\", \"/\"]):\n",
        "        decision = \"use_tool\"\n",
        "\n",
        "    # Rule-based document keyword detection\n",
        "    elif any(keyword in question for keyword in [\n",
        "        \"artificial intelligence\",\n",
        "        \"machine learning\",\n",
        "        \"deep learning\",\n",
        "        \"natural language processing\",\n",
        "        \"langgraph\",\n",
        "        \"faiss\",\n",
        "        \"rag\",\n",
        "        \"vector embeddings\",\n",
        "        \"stateful agent\",\n",
        "        \"conditional edges\"\n",
        "    ]):\n",
        "        decision = \"use_rag\"\n",
        "\n",
        "    else:\n",
        "        decision = \"no_tool\"\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"decision\": decision\n",
        "    }"
      ],
      "metadata": {
        "id": "XpcoMKfsJ0hB"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decision node is updated from Part I to now handle three routing paths instead of two. The LLM reads the question and decides whether to use the calculator, retrieve from the document using RAG, or answer directly without any tools."
      ],
      "metadata": {
        "id": "bcE16ZyQKLn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 4: Tool Node\n",
        "def tool_node(state: AgentState) -> AgentState:\n",
        "    question = state[\"question\"]\n",
        "    match = re.search(r\"\\d[\\d\\s\\+\\-\\*\\/\\.\\(\\)]*\", question)\n",
        "    expression = match.group().strip() if match else question\n",
        "    result = calculator.invoke({\"expression\": expression})\n",
        "    return {**state, \"tool_output\": result}"
      ],
      "metadata": {
        "id": "AqAwU0j7Ke0R"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tool node is carried over from Part I with the same regex extraction fix. It pulls only the mathematical expression from the question before passing it to the calculator, storing the result in tool_output within the state."
      ],
      "metadata": {
        "id": "Bbqoz11CKk0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 5: Retrieval Node\n",
        "\n",
        "def retrieval_node(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Retrieve relevant documents from vector store.\n",
        "    \"\"\"\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Perform similarity search and return top 3 chunks\n",
        "    docs = vectorstore.similarity_search(question, k=3)\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"retrieved_docs\": docs\n",
        "    }"
      ],
      "metadata": {
        "id": "p655GSPHKljp"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The retrieval node performs a similarity search on the FAISS vector store using the user's question. It fetches the top 3 most relevant chunks from the document and stores them in the retrieved_docs field of the state so the answer node can use them to generate a context-aware response."
      ],
      "metadata": {
        "id": "KKGBunplK0P7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 6: Updated Answer Node\n",
        "def answer_node(state: AgentState) -> AgentState:\n",
        "    question = state[\"question\"]\n",
        "    tool_output = state.get(\"tool_output\")\n",
        "    retrieved_docs = state.get(\"retrieved_docs\")\n",
        "\n",
        "    if tool_output:\n",
        "        final_answer = f\"The result is: {tool_output}\"\n",
        "    elif retrieved_docs:\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "        prompt = f\"\"\"Answer ONLY using the context below. Do not use outside knowledge.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "        response = llm.invoke([HumanMessage(content=prompt)])\n",
        "        final_answer = response.content\n",
        "    else:\n",
        "        response = llm.invoke([HumanMessage(content=question)])\n",
        "        final_answer = response.content\n",
        "\n",
        "    return {**state, \"final_answer\": final_answer}"
      ],
      "metadata": {
        "id": "LOHyiKOwKzvi"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The answer node now handles three cases. If a calculator result exists it formats it directly, if retrieved documents exist it injects them as context into the LLM prompt to generate an informed answer, and if neither exists it falls back to answering directly with the LLM."
      ],
      "metadata": {
        "id": "li66FgD9LB80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 7: Conditional Routing Function\n",
        "\n",
        "def route_decision(state: AgentState):\n",
        "    \"\"\"\n",
        "    Route based on decision.\n",
        "    \"\"\"\n",
        "    if \"use_tool\" in state[\"decision\"]:\n",
        "        return \"tool_node\"\n",
        "    elif \"use_rag\" in state[\"decision\"]:\n",
        "        return \"retrieval_node\"\n",
        "    else:\n",
        "        return \"answer_node\""
      ],
      "metadata": {
        "id": "2f8VFBMlLDEZ"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The routing function is updated from Part I to handle three paths instead of two. Using in instead of == makes the routing more robust since Mistral sometimes returns extra words along with the decision keyword, which would break exact string matching."
      ],
      "metadata": {
        "id": "XE8v_3RqLQHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 8: Build the Updated Graph\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"decision_node\", decision_node)\n",
        "workflow.add_node(\"tool_node\", tool_node)\n",
        "workflow.add_node(\"retrieval_node\", retrieval_node)\n",
        "workflow.add_node(\"answer_node\", answer_node)\n",
        "\n",
        "workflow.set_entry_point(\"decision_node\")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"decision_node\",\n",
        "    route_decision,\n",
        "    {\n",
        "        \"tool_node\": \"tool_node\",\n",
        "        \"retrieval_node\": \"retrieval_node\",\n",
        "        \"answer_node\": \"answer_node\"\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"tool_node\", \"answer_node\")\n",
        "workflow.add_edge(\"retrieval_node\", \"answer_node\")\n",
        "workflow.add_edge(\"answer_node\", END)\n",
        "\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "MQZ0x5JWLaPe"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph is updated from Part I to include the new retrieval_node. Both tool_node and retrieval_node now connect to answer_node, and the conditional edges handle all three routing paths. The graph is compiled into the final runnable app."
      ],
      "metadata": {
        "id": "fj7CaSmCLhvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO 9: Run the Agent\n",
        "\n",
        "# Test 1: Math question uses calculator tool\n",
        "result1 = app.invoke({\n",
        "    \"question\": \"What is 39 * 4 + 10?\",\n",
        "    \"decision\": None,\n",
        "    \"tool_output\": None,\n",
        "    \"retrieved_docs\": None,\n",
        "    \"final_answer\": None\n",
        "})\n",
        "print(\"Test 1 - Math Question:\")\n",
        "print(result1[\"final_answer\"])\n",
        "\n",
        "# Test 2: Document question using above trained text\n",
        "result2 = app.invoke({\n",
        "    \"question\": \"What is LangGraph?\",\n",
        "    \"decision\": None,\n",
        "    \"tool_output\": None,\n",
        "    \"retrieved_docs\": None,\n",
        "    \"final_answer\": None\n",
        "})\n",
        "print(\"\\nTest 2 - Document Question:\")\n",
        "print(result2[\"final_answer\"])\n",
        "\n",
        "# Test 3: General question answers directly\n",
        "result3 = app.invoke({\n",
        "    \"question\": \"What is the capital of India?\",\n",
        "    \"decision\": None,\n",
        "    \"tool_output\": None,\n",
        "    \"retrieved_docs\": None,\n",
        "    \"final_answer\": None\n",
        "})\n",
        "print(\"\\nTest 3 - General Question:\")\n",
        "print(result3[\"final_answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N156prLVLpSb",
        "outputId": "359f83d1-dcf6-4620-a728-47459df3f3fd"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1 - Math Question:\n",
            "The result is: 166\n",
            "\n",
            "Test 2 - Document Question:\n",
            " LangGraph is a framework for building stateful multi-agent AI applications, where each agent maintains context and memory across multiple interaction steps. It incorporates conditional edges to allow dynamic routing based on agent decisions.\n",
            "\n",
            "Test 3 - General Question:\n",
            " The capital of India is New Delhi. It serves as the political and administrative center of the country, while Mumbai is its financial hub and Bangalore is known for being a major information technology (IT) hub.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Changes from Part I:**\n",
        "\n",
        "The decision node was updated to handle three routing paths instead of two, adding use_rag as a new option alongside use_tool and no_tool. A new retrieval_node was added that performs similarity search on the FAISS vector store and stores the top 3 relevant chunks in the state. The answer node was updated to handle three cases:\n",
        "* calculator result\n",
        "* retrieved document context\n",
        "* direct LLM response.\n",
        "\n",
        "The routing function was updated to use \"in\" instead of \"==\" to handle cases where Mistral returns extra words along with the decision keyword.\n",
        "\n",
        "**Reflection:**\n",
        "\n",
        "Adding RAG to the agent made it significantly more capable since it can now answer questions grounded in a specific document rather than relying purely on what the LLM already knows.\n",
        "\n",
        "The biggest challenge was getting Mistral to strictly follow the context and not fall back to its own knowledge. The routing also needed to be made more flexible since the model does not always return a clean single keyword decision."
      ],
      "metadata": {
        "id": "vpJXkJvbaDs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why use LangGraph instead of a simple agent?**\n",
        "\n",
        "LangGraph provides a structured and stateful workflow instead of a single prompt-response interaction. It allows clear separation between decision-making, tool execution, retrieval, and answer generation. In Part I, it enabled conditional routing between calculator and direct responses. In Part II, it supported clean integration of RAG with multi-branch routing. Overall, it improves modularity, debugging, and scalability compared to a simple agent.\n",
        "\n",
        "**What limitations did you observe?**\n",
        "\n",
        "LLM-based routing can be sensitive and may misclassify general questions as document-based without careful logic. RAG performance depends heavily on embedding similarity and chunk quality, which can affect retrieval accuracy. The model may combine multiple retrieved chunks and produce verbose answers. Additionally, the system maintains state only within a session and does not provide persistent long-term memory by default."
      ],
      "metadata": {
        "id": "JVRRlGEOsDAb"
      }
    }
  ]
}