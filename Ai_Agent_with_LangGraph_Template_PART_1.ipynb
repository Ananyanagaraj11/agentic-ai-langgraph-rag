{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CIS 600 Applied Agetic AI Systems\n",
        "Spring 2026\n",
        "\n",
        "Assignment II\n"
      ],
      "metadata": {
        "id": "GmlhqqPXDbo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph langchain langchain-community langchain-core"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnHdWQSXulfO",
        "outputId": "a5e05d7a-3d06-4444-ec65-443e42cf4b40"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.8)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.10)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.2.13)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.6)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.12.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.46)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.4)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.13.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.7.3)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (26.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.14.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph) (1.12.2)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y zstd\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2wvx4EfvYcF",
        "outputId": "73bf33c7-72f6-45b1-93e3-afd1ae66eb4d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "zstd is already the newest version (1.4.8+dfsg-3build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading ollama-linux-amd64.tar.zst\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess, time\n",
        "\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "time.sleep(5)\n",
        "result = subprocess.run([\"ollama\", \"pull\", \"llama3.2\"], capture_output=False)\n",
        "print(\"Done!\", result.returncode)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6SpZ1u0v0SN",
        "outputId": "45fd4a51-1d86-42a8-bbfa-61f0cee27792"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "res = requests.post(\"http://localhost:11434/api/generate\",\n",
        "    json={\"model\": \"llama3.2\", \"prompt\": \"Say hello\", \"stream\": False})\n",
        "print(res.json()[\"response\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOTd-ct9woQv",
        "outputId": "6104a924-e69b-4344-b15c-d3ee46b88495"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* First, the required Python libraries were installed such as langgraph, langchain, langchain-community, and langchain-core since Colab doesn't have these by default.\n",
        "* Ollama couldn't be installed straight away because the newer version needs zstd for extraction, so that was installed first using apt-get before re-running the Ollama install script.\n",
        "* Once Ollama was installed, the server was started in the background using Python's subprocess module, and the mistral model was pulled since that is the default LLM mentioned in the template and what will be used throughout this assignment.\n",
        "* Finally, a quick test was run by sending a prompt to the local Ollama API endpoint to confirm the server was up and the model was responding correctly before writing any agent code."
      ],
      "metadata": {
        "id": "_G1vOcn-xkly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subprocess.run([\"ollama\", \"pull\", \"mistral\"], capture_output=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy__oq-NzAhe",
        "outputId": "8269f74d-93b4-4109-b645-316ce1947653"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['ollama', 'pull', 'mistral'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Optional\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "# OR use HuggingFaceHub"
      ],
      "metadata": {
        "id": "fZeJhMJIDqWj"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "8opPcxBvDWCf"
      },
      "outputs": [],
      "source": [
        "\n",
        "#################################################\n",
        "# TODO 1: Initialize Your Free LLM\n",
        "#################################################\n",
        "\n",
        "# Option A: Ollama (recommended if installed locally)\n",
        "llm = ChatOllama(model=\"mistral\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializes the free LLM using ChatOllama with the Mistral model running locally through Ollama. This is the core language model that powers all decision-making and answer generation in the agent."
      ],
      "metadata": {
        "id": "DzQSKpo56sNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#################################################\n",
        "# TODO 2: Define Agent State\n",
        "#################################################\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    question: str\n",
        "    decision: Optional[str]\n",
        "    tool_input: Optional[str]\n",
        "    tool_output: Optional[str]\n",
        "    final_answer: Optional[str]\n",
        "\n"
      ],
      "metadata": {
        "id": "JdTb8dLBD773"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines the AgentState TypedDict which acts as shared memory across all nodes.\n",
        "\n",
        "It holds the question, decision, tool input/output, and final answer throughout the workflow."
      ],
      "metadata": {
        "id": "xmm42UIK61SS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################\n",
        "# TODO 3: Create a Tool\n",
        "#################################################\n",
        "\n",
        "@tool\n",
        "def calculator(expression: str) -> str:\n",
        "    \"\"\"\n",
        "    Evaluate a basic mathematical expression.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return str(eval(expression))\n",
        "    except Exception:\n",
        "        return \"Error in calculation.\"\n"
      ],
      "metadata": {
        "id": "pNsYqClyEMkT"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates a calculator tool using the @tool decorator that evaluates a mathematical expression using Python's eval().\n",
        "\n",
        "It returns the result as a string or an error message if the expression is invalid."
      ],
      "metadata": {
        "id": "qKd5u_886-J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################\n",
        "# TODO 4: Decision Node\n",
        "#################################################\n",
        "\n",
        "def decision_node(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Decide whether to use tool or answer directly.\n",
        "    \"\"\"\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a decision-making agent.\n",
        "    If the question requires math calculation, output: use_tool\n",
        "    Otherwise output: no_tool\n",
        "\n",
        "    Question: {question}\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke([HumanMessage(content=prompt)])\n",
        "\n",
        "    # Normalize decision to avoid fragile exact string matching\n",
        "    raw = response.content.strip().lower()\n",
        "    if \"use_tool\" in raw:\n",
        "        decision = \"use_tool\"\n",
        "    else:\n",
        "        decision = \"no_tool\"\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"decision\": decision\n",
        "    }"
      ],
      "metadata": {
        "id": "DiG3kURPETqL"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decision node prompts the LLM to decide whether the question needs the calculator tool or can be answered directly.\n",
        "\n",
        "The raw response is normalized using a lowercase check with \"in\" instead of exact string matching to handle cases where Mistral returns extra words alongside the keyword, making routing more robust."
      ],
      "metadata": {
        "id": "cB3iNM8h7HPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################\n",
        "# TODO 5: Tool Node\n",
        "#################################################\n",
        "\n",
        "import re\n",
        "\n",
        "def tool_node(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Executes the calculator tool.\n",
        "    \"\"\"\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Remove everything except valid math characters\n",
        "    expression = re.sub(r\"[^\\d\\.\\+\\-\\*\\/\\(\\)]\", \"\", question)\n",
        "\n",
        "    result = calculator.invoke(expression)\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"tool_input\": expression,\n",
        "        \"tool_output\": result\n",
        "    }"
      ],
      "metadata": {
        "id": "pL7pqtOHEYbv"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tool node extracts the math expression from the question using regex and passes it to the calculator tool.\n",
        "\n",
        "The result is stored back into the state as tool_output.\n",
        "\n",
        "The template directly passed the full question string to calculator.invoke(question), which caused an \"Error in calculation\".\n",
        "\n",
        "This was fixed by adding regex extraction to pull only the mathematical expression before passing it to the calculator."
      ],
      "metadata": {
        "id": "58HrLPPU7Vu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################\n",
        "# TODO 6: Final Answer Node\n",
        "#################################################\n",
        "\n",
        "def answer_node(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Generate final answer.\n",
        "    \"\"\"\n",
        "\n",
        "    question = state[\"question\"]\n",
        "    tool_output = state.get(\"tool_output\", \"\")\n",
        "\n",
        "    # TODO:\n",
        "    # If tool_output exists, use it in final answer.\n",
        "    # Otherwise answer directly with LLM.\n",
        "\n",
        "    if tool_output:\n",
        "        final_answer = f\"The result is: {tool_output}\"\n",
        "    else:\n",
        "        response = llm.invoke([HumanMessage(content=question)])\n",
        "        final_answer = response.content\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"final_answer\": final_answer\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Lbr3uU4lEc9k"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The answer node checks whether a tool_output exists and formats the final response accordingly.\n",
        "\n",
        "If no tool was used, it calls the LLM directly to generate an answer"
      ],
      "metadata": {
        "id": "X0QCXBdl7hEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################\n",
        "# TODO 7: Conditional Routing Function\n",
        "#################################################\n",
        "\n",
        "def route_decision(state: AgentState):\n",
        "    \"\"\"\n",
        "    Route based on decision.\n",
        "    \"\"\"\n",
        "    if state[\"decision\"] == \"use_tool\":\n",
        "        return \"tool_node\"\n",
        "    else:\n",
        "        return \"answer_node\"\n"
      ],
      "metadata": {
        "id": "qGnAAu0vEiXM"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The routing function reads the decision from state and returns either \"tool_node\" or \"answer_node\" as the next step. This is what drives the conditional branching in the graph."
      ],
      "metadata": {
        "id": "7MkscBrj7oYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################\n",
        "# TODO 8: Build the Graph\n",
        "#################################################\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"decision_node\", decision_node)\n",
        "workflow.add_node(\"tool_node\", tool_node)\n",
        "workflow.add_node(\"answer_node\", answer_node)\n",
        "\n",
        "workflow.set_entry_point(\"decision_node\")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"decision_node\",\n",
        "    route_decision,\n",
        "    {\n",
        "        \"tool_node\": \"tool_node\",\n",
        "        \"answer_node\": \"answer_node\"\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"tool_node\", \"answer_node\")\n",
        "workflow.add_edge(\"answer_node\", END)\n",
        "\n",
        "app = workflow.compile()\n",
        "\n"
      ],
      "metadata": {
        "id": "5n1FBodgEmP3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Builds and compiles the full LangGraph workflow by registering all nodes, setting the entry point, and wiring the conditional and regular edges together."
      ],
      "metadata": {
        "id": "gr80xDle7wEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################\n",
        "# TODO 9: Run the Agent\n",
        "#################################################\n",
        "\n",
        "# Test 1: Math question uses calculator tool\n",
        "result1 = app.invoke({\n",
        "    \"question\": \"What is 39 * 4 + 10?\",\n",
        "    \"decision\": None,\n",
        "    \"tool_input\": None,\n",
        "    \"tool_output\": None,\n",
        "    \"final_answer\": None\n",
        "})\n",
        "print(\"Test 1 - Math Question:\")\n",
        "print(result1[\"final_answer\"])\n",
        "\n",
        "# Test 2: General question which answers directly\n",
        "result2 = app.invoke({\n",
        "    \"question\": \"What is the Capital of India?\",\n",
        "    \"decision\": None,\n",
        "    \"tool_input\": None,\n",
        "    \"tool_output\": None,\n",
        "    \"final_answer\": None\n",
        "})\n",
        "print(\"Test 2 - General Question:\")\n",
        "print(result2[\"final_answer\"])"
      ],
      "metadata": {
        "id": "bQHEoJUfEqlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90094f70-2713-4cca-f5da-f1ffafb5192f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1 - Math Question:\n",
            "The result is: 166\n",
            "Test 2 - General Question:\n",
            " The capital of India is New Delhi. It serves as both a city and a union territory in its own right, and it's divided into two parts: Old Delhi (North) and New Delhi (South). New Delhi is known for landmarks that reflect Indian history and modernization like the Red Fort, Qutub Minar, Humayun's Tomb, India Gate, and the Parliament House.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Runs two test cases through the compiled agent\n",
        "* one math question\n",
        "* one general question\n",
        "\n",
        "To verify both routing paths work correctly.\n",
        "\n",
        "The template used input() to accept questions from the user at runtime, which does not work well in Google Colab.\n",
        "So I replaced it with two hardcoded test questions one math question and one general question to demonstrate both routing paths of the agent clearly."
      ],
      "metadata": {
        "id": "_uQClKEy8cJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tools used:\n",
        "\n",
        "\n",
        "\n",
        "* LLM: Mistral through ChatOllama\n",
        "* Framework: LangGraph StateGraph\n",
        "* Tool: Calculator\n",
        "* Nodes: decision_node, tool_node, answer_node\n",
        "* Conditional Edge: decision_node to tool_node or answer_node\n",
        "* State: AgentState TypedDict\n",
        "* Deliverables: .ipynb with outputs and reflection\n",
        "\n",
        "**Why use LangGraph instead of a simple agent?**\n",
        "\n",
        "When I first started building this, a simple LLM call felt enough. You send a question, you get an answer. But the problem is that a basic LLM call has no memory of what happened before it and no structure to follow. LangGraph changes that by letting it define the workflow as a graph where each node has a specific job and the state carries information from one step to the next. The other reason is conditional routing. With a plain LLM call it would have to write our own logic outside the model to decide what happens next. LangGraph handles that natively through conditional edges which makes the whole flow cleaner and easier to debug.\n",
        "\n",
        "**What limitations did you observe?**\n",
        "\n",
        "The biggest issue I ran into was with the decision node. Mistral sometimes returned a full sentence instead of just use_tool or no_tool which broke the routing. This was fixed by normalizing the response using a lowercase \"in\" check instead of exact string matching. The second limitation was speed  running Mistral on CPU in Colab with no GPU meant every LLM call took a couple of minutes, which would be a serious bottleneck in any real application.\n",
        "\n"
      ],
      "metadata": {
        "id": "Nk6_Zin5Aq__"
      }
    }
  ]
}